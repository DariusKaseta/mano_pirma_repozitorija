{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup\n",
    "\n",
    "Beautiful Soup yra Python biblioteka, duomenų traukimui iš HTML ir XML. BS moka naviguoti tekste pagal html blokus, klases, id ar kitus atributus. Vienas iš populiariausių įrankių web scraping užduotims atlikti. Dokumentaciją rasite čia. Diegiasi pip install beautifulsoup4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <title>First HTML Page</title>\n",
    "</head>\n",
    "<body>\n",
    "  <div id=\"first\">\n",
    "    <h3 data-example=\"yes\">hi</h3>\n",
    "    <p>more text.</p>\n",
    "  </div>\n",
    "  <ol>\n",
    "    <li class=\"special\">This list item is special.</li>\n",
    "    <li class=\"special\">This list item is also special.</li>\n",
    "    <li class=\"not-special\">This list item is not special.</li>\n",
    "  </ol>\n",
    "  <div data-example=\"yes\">bye</div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>\n",
      "<div id=\"first\">\n",
      "<h3 data-example=\"yes\">hi</h3>\n",
      "<p>more text.</p>\n",
      "</div>\n",
      "<ol>\n",
      "<li class=\"special\">This list item is special.</li>\n",
      "<li class=\"special\">This list item is also special.</li>\n",
      "<li>This list item is not special.</li>\n",
      "</ol>\n",
      "<div data-example=\"yes\">bye</div>\n",
      "</body>\n"
     ]
    }
   ],
   "source": [
    "# Importuojame BeautifulSoup klasę, ir iš HTML stringo išsiverdame buljoną. \n",
    "# Dabar, turėdami soup objektą, galime naudotis jo funkcionalumu duomenų traukimui. Pvz.:\n",
    "\n",
    "print(soup.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"first\">\n",
      "<h3 data-example=\"yes\">hi</h3>\n",
      "<p>more text.</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# Ištraukiame body bloką. Arba:\n",
    "\n",
    "print(soup.body.div)\n",
    "\n",
    "# Gavome pirmą div bloką bloke body."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # find(), find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"first\">\n",
      "<h3 data-example=\"yes\">hi</h3>\n",
      "<p>more text.</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# Lygiai tą patį gautumėm panaudoję .find() metodą:\n",
    "print(soup.find('div'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div id=\"first\">\n",
      "<h3 data-example=\"yes\">hi</h3>\n",
      "<p>more text.</p>\n",
      "</div>, <div data-example=\"yes\">bye</div>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all('div'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"first\">\n",
      "<h3 data-example=\"yes\">hi</h3>\n",
      "<p>more text.</p>\n",
      "</div>\n",
      "<div data-example=\"yes\">bye</div>\n"
     ]
    }
   ],
   "source": [
    "for div in soup.find_all('div'):\n",
    "    print(div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li class=\"special\">This list item is special.</li>, <li class=\"special\">This list item is also special.</li>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all(class_='special'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h3 data-example=\"yes\">hi</h3>, <div data-example=\"yes\">bye</div>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all(attrs={'data-example': 'yes'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div id=\"first\">\n",
      "<h3 data-example=\"yes\">hi</h3>\n",
      "<p>more text.</p>\n",
      "</div>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.select('#first'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li class=\"special\">This list item is special.</li>, <li class=\"special\">This list item is also special.</li>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.select('.special'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h3 data-example=\"yes\">hi</h3>, <div data-example=\"yes\">bye</div>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.select('[data-example=\"yes\"]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h3 hi\n",
      "{'data-example': 'yes'}\n",
      "div bye\n",
      "{'data-example': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "yes_list = soup.select('[data-example=\"yes\"]')\n",
    "for yes in yes_list:\n",
    "    print(yes.name, yes.get_text())\n",
    "    print(yes.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first\n"
     ]
    }
   ],
   "source": [
    "attribute = soup.select('div')[0]['id']\n",
    "print(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', <h3 data-example=\"yes\">hi</h3>, '\\n', <p>more text.</p>, '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(soup.div.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li class=\"special\">This list item is also special.</li>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = soup.find('li')\n",
    "li.next_sibling.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ol>\n",
       "<li class=\"special\">This list item is special.</li>\n",
       "<li class=\"special\">This list item is also special.</li>\n",
       "<li class=\"not-special\">This list item is not special.</li>\n",
       "</ol>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<li class=\"special\">This list item is also special.</li>,\n",
       " <li class=\"not-special\">This list item is not special.</li>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li.find_next_siblings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li class=\"not-special\">This list item is not special.</li>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = soup.find('li')\n",
    "li.find_next_sibling(class_='not-special')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first\n"
     ]
    }
   ],
   "source": [
    "li = soup.find('li')\n",
    "res = li.find_parent().find_previous_sibling()['id']\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "res = soup.body.next_element.next_element.next_element.next_element.get_text()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HTML5'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('https://en.wikipedia.org/wiki/HTML5')\n",
    "if response.status_code == 200:\n",
    "    html5 = response.content\n",
    "html5_soup = BeautifulSoup(html5, 'html.parser')\n",
    "html5_soup.select('h1')[0].get_text()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uzduotys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antaste su dvitaskiu:\n",
      "\n",
      "Antraste be dvitaskio:\n",
      "Antaste su dvitaskiu:\n",
      "\n",
      "Antraste be dvitaskio:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Parašykite programą, kuri nuskaitytų delfi antraštes, patikrintų, ar jos turi dvitaškį. \n",
    "# Dalį iki dvitaškio sudėtų į vieną sarašą, dalį po dvitaškio į kitą. Antrą sarašą išmaišykite (google). \n",
    "# Tuomet atspausdinkite pirmas dalis iš pirmo sarašo, prie jų prijunkite antras dalis iš antro sąrašo. Turėtume gauti panašių variantų:\n",
    "\n",
    "# Orai : už 9 šlakius teks sumokėti 26 tūkstančius eurų\n",
    "# Antradienio vakare kauniečius išgąsdino termofikacijos elektrinė : ar bus naujagimių bumas?\n",
    "\n",
    "# Sukurkite blogų žodžių sąrašą, pagal kurį išsifiltruoja pranešimai apie COVID, mirtis ir t.t. \n",
    "# Išfiltruokite ankstyvoje stadijoje, kol dar antraštės neperskirtos.\n",
    "\n",
    "from random import shuffle, random\n",
    "import re\n",
    "\n",
    "def scrape_headlines():\n",
    "    url = \"https://www.delfi.lt\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    headlines = soup.find_all(\"a\", class_=re.compile(\"headline-link.*\"))\n",
    "\n",
    "    headlines_with_colon = []\n",
    "    headlines_without_colon = []\n",
    "    \n",
    "    for headline in headlines:\n",
    "        title = headline.text.strip()\n",
    "        if \":\" in title:\n",
    "            headlines_with_colon.append(title)\n",
    "        else:\n",
    "            headlines_without_colon.append(title)\n",
    "    print(\"Antaste su dvitaskiu:\")\n",
    "    for title in headlines_with_colon:\n",
    "        print(title)\n",
    "    print(\"\\nAntraste be dvitaskio:\")\n",
    "    for title in headlines_without_colon:\n",
    "        print(title)\n",
    "    \n",
    "    both = []\n",
    "    for i in range(min(len(headlines_with_colon), len(headlines_without_colon))):\n",
    "        first_part = headlines_with_colon[i].split(\":\")[0]\n",
    "        second_part = headlines_without_colon[i].split(\":\")[1]\n",
    "        if first_part and second_part:\n",
    "            both.append((first_part, second_part))\n",
    "    \n",
    "    return both\n",
    "    # print(first_part + \":\" + second_part)\n",
    "\n",
    "result = scrape_headlines()\n",
    "if result:\n",
    "    for pair in result:\n",
    "        print(pair[0] + \":\" + pair[1])\n",
    "    else:\n",
    "        print(\"Nera tinkamu poru antrasciu\")\n",
    "\n",
    "scrape_headlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3613020507.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[69], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    # print(f\"{first_part[i]}:{second_part[i]}\")\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "html = requests.get(\"https://www.delfi.lt\").text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "title_tags = soup.select(\".CBarticleTitle\")\n",
    "titles = [i.get_text() for i in title_tags]\n",
    "bad_words = [\"COVID\", \"mirtis\", \"NVSC\", \"skiepai\", \"privaloma\"]\n",
    "\n",
    "first_part = []\n",
    "second_part = []\n",
    "for title in titles:\n",
    "    if \":\" in title:\n",
    "        if not any(word in title for word in bad_words):\n",
    "            splitted = title.split(\":\")\n",
    "            first_part.append(splitted[0])\n",
    "            second_part.append(splitted[1])\n",
    "\n",
    "shuffle(second_part)\n",
    "\n",
    "for i in range(len(first_part)):\n",
    "    # print(f\"{first_part[i]}:{second_part[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "Correct! Answer is Albert Einstein\n"
     ]
    }
   ],
   "source": [
    "# Parašykite žaidimą, kuris iš svetainės http://quotes.toscrape.com/ pateiks citatas, o žaidėjui reikės atspėti autorių. \n",
    "# Žaidėjui neatspėjus, reikės pasufleruoti autoriaus inicialus, dar kartą neatspėjus - gimimo datą ir vietą. \n",
    "# Jeigu žaidėjas neatspėja iš 3 kartų, jam atspausdinamas teisingas atsakymas ir paklausiama, ar nori tęsti.\n",
    "from random import randint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://quotes.toscrape.com/\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "quotes = soup.select(\".text\")\n",
    "quotes_list = [i.get_text() for i in quotes]\n",
    "\n",
    "blocks = soup.find_all('a', attrs={'class': None})\n",
    "hrefs = [i[\"href\"] for i in blocks if i.get_text()==\"(about)\"]\n",
    "\n",
    "autohors_blocks = soup.find_all(\"small\", class_=\"author\")\n",
    "answers = [i.get_text() for i in autohors_blocks]\n",
    "\n",
    "hint1 = []\n",
    "for i in answers:\n",
    "    splitted = i.split()\n",
    "    hint = \"\"\n",
    "    for i in splitted:\n",
    "        if \".\" not in i:\n",
    "            hint += f\"{i[0]}.\"\n",
    "        else:\n",
    "            hint += i\n",
    "    hint1.append(hint)\n",
    "\n",
    "def get_second_hint(i):\n",
    "    r = requests.get(url + hrefs[i])\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    text = soup.select('p')[1].get_text()\n",
    "    return text\n",
    "\n",
    "while True:\n",
    "    i = randint(0, 9)\n",
    "    print('\\n',quotes_list[i])\n",
    "    answer1 = input('Your answer: ')\n",
    "    if answer1 == answers[i]:\n",
    "        print(f\"Correct! Answer is {answers[i]}\")\n",
    "    else:\n",
    "        print(hint1[i])\n",
    "        answer2 = input('Your answer: ')\n",
    "        if answer2 == answers[i]:\n",
    "            print(f\"Correct! Answer is {answers[i]}\")\n",
    "        else:\n",
    "            print(get_second_hint(i))\n",
    "            answer3 = input('Your answer: ')\n",
    "            if answer3 == answers[i]:\n",
    "                print(f\"Correct! Answer is {answers[i]}\")\n",
    "            else:\n",
    "                print(f\"Wrong! Correct answer is {answers[i]}\")\n",
    "    if_continue = input('Continue? y/n: ')\n",
    "    if if_continue != 'y':\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
